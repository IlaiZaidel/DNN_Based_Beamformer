from torch.utils.data import Dataset,DataLoader
import pandas as pd
import soundfile as sf
import torch
import torchaudio
import torchaudio.functional as F
import numpy as np
from hrtf_convolve import SOFA_HRTF_wrapper
from omegaconf import OmegaConf
from model import ComplexExtraction
from glob import glob
from random import shuffle
import random

class ExtractionDataset(Dataset):
    def __init__(self,hp,train=True):
        self.hp = hp
        self.df = pd.read_csv(self.hp.dataset.csv_path) if train else pd.read_csv(self.hp.dataset.test_csv_path) 
        self.fs = self.hp.dataset.fs
        self.hrtf_obj = SOFA_HRTF_wrapper(path=hp.dataset.hrtf_path) #,var=hp.dataset.var_hrtf)

    def __len__(self):
        return len(self.df)
    
    def preprocess(self,x,fs):
        #cut length
        if fs != self.fs:
            x = F.resample(x, fs, self.fs)
        if x.shape[0]!=1:
            x = x.squeeze()
        if self.hp.dataset.norm_sample:
            x = x/(x.abs().max(dim=-1, keepdim=True).values)
        if x.shape[-1] <= self.hp.dataset.time_len*self.fs:
             #cat zeros
            remain = self.hp.dataset.time_len*self.fs-x.shape[-1]
            x = torch.cat((x,torch.zeros((x.shape[0],remain))),-1)
        else:
            #cut
            x = x[:,:self.hp.dataset.time_len*self.fs] 
        return x
    
    def mix(self,x1,x2):
        x = x1+x2
        x = x/(x.abs().max(dim=-1, keepdim=True).values)
        return self.stft_sample(x)
    
    def stft_sample(self,x):
        X = torch.stft(torch.squeeze(x),n_fft=self.hp.stft.fft_length, hop_length=self.hp.stft.fft_hop, window=torch.hann_window(self.hp.stft.fft_length), return_complex=True).to(torch.complex64)
        X[0:2, :] = X[0:2, :] * 0.001
        mx_x = torch.max(torch.max(torch.abs(torch.real(X)) , torch.max(torch.abs(torch.imag(X)) )))
        X = X/mx_x
        return X
    
    def hrtf_preprocces(self,hrtf):
        HRTF = torch.fft.fft(hrtf,self.hp.stft.fft_length).to(torch.complex64)
        HRTF = HRTF[:,1:self.hp.stft.fft_length//2 +1]
        return HRTF

    def get_hrtf(self,az,elev):
        h=self.hrtf_obj.get_hrtf(az,elev,fs=self.hp.dataset.fs)
        h = self.hrtf_preprocces(torch.tensor(h.T))
        return h
    
    def __getitem__(self, idx):
        line = self.df.iloc[idx]
        # x1,fs_x1 = torchaudio.load(line['speaker_1'])
        x1_hrtf,fs_x1,hrtf1,real_az1,real_elev1 = self.hrtf_obj.conv_file(line['speaker_1'],az=line['az_1'],elev=line['elev_1'])
        x2_hrtf,fs_x2,hrtf2,real_az2,real_elev2 = self.hrtf_obj.conv_file(line['speaker_2'],az=line['az_2'],elev=line['elev_2'])
        x1_hrtf,x2_hrtf = self.preprocess(x1_hrtf,fs_x1),self.preprocess(x2_hrtf,fs_x2)
        hrtf1_time,hrtf2_time = hrtf1,hrtf2
        hrtf1,hrtf2 = self.hrtf_preprocces(hrtf1),self.hrtf_preprocces(hrtf2)
        Mix = self.mix(x1_hrtf,x2_hrtf)
        if self.hp.output_channels==1:
            wav1,fs = sf.read(line['speaker_1'])
            x1 = self.preprocess(torch.tensor(wav1).unsqueeze(0),fs)
            wav2,fs = sf.read(line['speaker_2'])
            x2 = self.preprocess(torch.tensor(wav2).unsqueeze(0),fs)
            Y1 = self.stft_sample(x1).unsqueeze(0)
            Y2 = self.stft_sample(x2).unsqueeze(0)
        else: 
            Y1 = self.stft_sample(x1_hrtf)
            Y2 = self.stft_sample(x2_hrtf)
        return Mix,Y1,Y2,hrtf1,hrtf2,hrtf1_time,hrtf2_time,real_az1,real_elev1,real_az2,real_elev2

    def iSTFT(self,x):
        x_time = torch.istft(torch.squeeze(x).detach().cpu(),n_fft=self.hp.stft.fft_length, hop_length=self.hp.stft.fft_hop, window=torch.hann_window(self.hp.stft.fft_length))
        x_time = x_time/(x_time.abs().max(dim=-1, keepdim=True).values)
        return x_time
    

class ExtractionFilesDataset(ExtractionDataset):
    def __init__(self, hp, train=False):
        super().__init__(hp, train)

    def __len__(self):
        return 1
    def __getitem__(self, idx):
        mix,sr = sf.read('/home/workspace/yoavellinson/extraction_master/complex_nn/extraction/mix_test.wav')
        mix = self.preprocess(torch.tensor(mix).T,sr)
        Mix = self.stft_sample(mix)
        hrtf1 = self.get_hrtf(90,0)
        hrtf2 = self.get_hrtf(340,0)

        return Mix,hrtf1,hrtf2



class ExtractionNoiseDataset(Dataset):
    def __init__(self,hp,train=True):
        self.hp = hp
        self.df = pd.read_csv(self.hp.dataset.csv_path) if train else pd.read_csv(self.hp.dataset.test_csv_path) 
        self.fs = self.hp.dataset.fs
        self.hrtf_obj = SOFA_HRTF_wrapper(path=hp.dataset.hrtf_path) #,var=hp.dataset.var_hrtf)
        self.noise_files = glob(hp.dataset.noise_dir+'/*.wav')
        shuffle(self.noise_files)
        self.az = list(range(0, 91,5)) + list(range(270, 361,5))
        self.tol = hp.dataset.noise_tol

    def __len__(self):
        return len(self.df)
    
    def preprocess(self,x,fs):
        #cut length
        if fs != self.fs:
            x = F.resample(x, fs, self.fs)
        if x.shape[0]!=1:
            x = x.squeeze()
        if self.hp.dataset.norm_sample:
            x = x/(x.abs().max(dim=-1, keepdim=True).values)
        if x.shape[-1] <= self.hp.dataset.time_len*self.fs:
             #cat zeros
            remain = self.hp.dataset.time_len*self.fs-x.shape[-1]
            x = torch.cat((x,torch.zeros((x.shape[0],remain))),-1)
        else:
            #cut
            x = x[:,:self.hp.dataset.time_len*self.fs] 
        return x
    
    def mix(self,x1,x2,n,snr_db):
        #add SNR here
        x = x1+x2
        g = self.calc_SNR_gain(x,n,snr_db)
        x += n*g
        x = x/(x.abs().max(dim=-1, keepdim=True).values)
        return self.stft_sample(x)
    
    def stft_sample(self,x):
        X = torch.stft(torch.squeeze(x),n_fft=self.hp.stft.fft_length, hop_length=self.hp.stft.fft_hop, window=torch.hann_window(self.hp.stft.fft_length), return_complex=True).to(torch.complex64)
        X[0:2, :] = X[0:2, :] * 0.001
        mx_x = torch.max(torch.max(torch.abs(torch.real(X)) , torch.max(torch.abs(torch.imag(X)) )))
        X = X/mx_x
        return X
    
    def hrtf_preprocces(self,hrtf):
        HRTF = torch.fft.fft(hrtf,self.hp.stft.fft_length).to(torch.complex64)
        HRTF = HRTF[:,1:self.hp.stft.fft_length//2 +1]
        return HRTF

    def get_hrtf(self,az,elev):
        h=self.hrtf_obj.get_hrtf(az,elev,fs=self.hp.dataset.fs)
        h = self.hrtf_preprocces(torch.tensor(h.T))
        return h
    
    def calc_SNR_gain(self,x,n,snr_db):
        x_power = torch.mean(x**2).sum()
        n_power = torch.mean(n**2).sum()
        snr_linear = 10**(snr_db/10)
        g = torch.sqrt(x_power/(snr_linear*n_power))
        return g
    
    def __getitem__(self, idx):
        line = self.df.iloc[idx]
        # x1,fs_x1 = torchaudio.load(line['speaker_1'])
        x1_hrtf,fs_x1,hrtf1,real_az1,real_elev1 = self.hrtf_obj.conv_file(line['speaker_1'],az=line['az_1'],elev=line['elev_1'])
        x2_hrtf,fs_x2,hrtf2,real_az2,real_elev2 = self.hrtf_obj.conv_file(line['speaker_2'],az=line['az_2'],elev=line['elev_2'])
        path = random.sample(self.noise_files,1)
        n_hrtf,fs_n,_,_,_ = self.hrtf_obj.conv_file(path[0],line['n_az'],line['n_elev'])
        n_hrtf = self.preprocess(n_hrtf,fs_n)
        x1_hrtf,x2_hrtf = self.preprocess(x1_hrtf,fs_x1),self.preprocess(x2_hrtf,fs_x2)
        hrtf1_time,hrtf2_time = hrtf1,hrtf2
        hrtf1,hrtf2 = self.hrtf_preprocces(hrtf1),self.hrtf_preprocces(hrtf2)

        snr_db =float(line['snr']) 
        Mix = self.mix(x1_hrtf,x2_hrtf,n_hrtf,snr_db)
        if self.hp.output_channels==1:
            wav1,fs = sf.read(line['speaker_1'])
            x1 = self.preprocess(torch.tensor(wav1).unsqueeze(0),fs)
            wav2,fs = sf.read(line['speaker_2'])
            x2 = self.preprocess(torch.tensor(wav2).unsqueeze(0),fs)
            Y1 = self.stft_sample(x1).unsqueeze(0)
            Y2 = self.stft_sample(x2).unsqueeze(0)
        else: 
            Y1 = self.stft_sample(x1_hrtf)
            Y2 = self.stft_sample(x2_hrtf)
        return Mix,Y1,Y2,hrtf1,hrtf2,hrtf1_time,hrtf2_time,real_az1,real_elev1,real_az2,real_elev2

    def iSTFT(self,x):
        x_time = torch.istft(torch.squeeze(x).detach().cpu(),n_fft=self.hp.stft.fft_length, hop_length=self.hp.stft.fft_hop, window=torch.hann_window(self.hp.stft.fft_length))
        x_time = x_time/(x_time.abs().max(dim=-1, keepdim=True).values)
        return x_time
    

class ExtractionDatasetSIR(Dataset):
    def __init__(self,hp,train=True):
        self.hp = hp
        self.df = pd.read_csv(self.hp.dataset.csv_path) if train else pd.read_csv(self.hp.dataset.test_csv_path) 
        self.fs = self.hp.dataset.fs
        self.hrtf_obj = SOFA_HRTF_wrapper(path=hp.dataset.hrtf_path) #,var=hp.dataset.var_hrtf)

    def __len__(self):
        return len(self.df)
    
    def preprocess(self,x,fs):
        #cut length
        if fs != self.fs:
            x = F.resample(x, fs, self.fs)
        if x.shape[0]!=1:
            x = x.squeeze()
        if self.hp.dataset.norm_sample:
            x = x/(x.abs().max(dim=-1, keepdim=True).values)
        if x.shape[-1] <= self.hp.dataset.time_len*self.fs:
             #cat zeros
            remain = self.hp.dataset.time_len*self.fs-x.shape[-1]
            x = torch.cat((x,torch.zeros((x.shape[0],remain))),-1)
        else:
            #cut
            x = x[:,:self.hp.dataset.time_len*self.fs] 
        return x
    
    def mix(self,x1,x2,snr_db):
        #add SNR here
        g = self.calc_SNR_gain(x1,x2,snr_db)
        x = x1+(g*x2)
        x = x/(x.abs().max(dim=-1, keepdim=True).values)
        return self.stft_sample(x)
    
    def stft_sample(self,x):
        X = torch.stft(torch.squeeze(x),n_fft=self.hp.stft.fft_length, hop_length=self.hp.stft.fft_hop, window=torch.hann_window(self.hp.stft.fft_length), return_complex=True).to(torch.complex64)
        X[0:2, :] = X[0:2, :] * 0.001
        mx_x = torch.max(torch.max(torch.abs(torch.real(X)) , torch.max(torch.abs(torch.imag(X)) )))
        X = X/mx_x
        return X
    
    def hrtf_preprocces(self,hrtf):
        HRTF = torch.fft.fft(hrtf,self.hp.stft.fft_length).to(torch.complex64)
        HRTF = HRTF[:,1:self.hp.stft.fft_length//2 +1]
        return HRTF

    def get_hrtf(self,az,elev):
        h=self.hrtf_obj.get_hrtf(az,elev,fs=self.hp.dataset.fs)
        h = self.hrtf_preprocces(torch.tensor(h.T))
        return h
    
    def calc_SNR_gain(self,x,n,snr_db):
        x_power = torch.mean(x**2).sum()
        n_power = torch.mean(n**2).sum()
        snr_linear = 10**(snr_db/10)
        g = torch.sqrt(x_power/(snr_linear*n_power))
        return g
    
    def __getitem__(self, idx):
        line = self.df.iloc[idx]
        # x1,fs_x1 = torchaudio.load(line['speaker_1'])
        x1_hrtf,fs_x1,hrtf1,real_az1,real_elev1 = self.hrtf_obj.conv_file(line['speaker_1'],az=line['az_1'],elev=line['elev_1'])
        x2_hrtf,fs_x2,hrtf2,real_az2,real_elev2 = self.hrtf_obj.conv_file(line['speaker_2'],az=line['az_2'],elev=line['elev_2'])
        x1_hrtf,x2_hrtf = self.preprocess(x1_hrtf,fs_x1),self.preprocess(x2_hrtf,fs_x2)
        hrtf1_time,hrtf2_time = hrtf1,hrtf2
        hrtf1,hrtf2 = self.hrtf_preprocces(hrtf1),self.hrtf_preprocces(hrtf2)

        snr_db =float(line['sir']) 
        Mix = self.mix(x1_hrtf,x2_hrtf,snr_db)
        if self.hp.output_channels==1:
            wav1,fs = sf.read(line['speaker_1'])
            x1 = self.preprocess(torch.tensor(wav1).unsqueeze(0),fs)
            wav2,fs = sf.read(line['speaker_2'])
            x2 = self.preprocess(torch.tensor(wav2).unsqueeze(0),fs)
            Y1 = self.stft_sample(x1).unsqueeze(0)
            Y2 = self.stft_sample(x2).unsqueeze(0)
        else: 
            Y1 = self.stft_sample(x1_hrtf)
            Y2 = self.stft_sample(x2_hrtf)
        return Mix,Y1,Y2,hrtf1,hrtf2,hrtf1_time,hrtf2_time,real_az1,real_elev1,real_az2,real_elev2

    def iSTFT(self,x):
        x_time = torch.istft(torch.squeeze(x).detach().cpu(),n_fft=self.hp.stft.fft_length, hop_length=self.hp.stft.fft_hop, window=torch.hann_window(self.hp.stft.fft_length))
        x_time = x_time/(x_time.abs().max(dim=-1, keepdim=True).values)
        return x_time
    

class BLCMVDataset(Dataset):
    def __init__(self,hp,train=True):
        self.hp = hp
        self.df = pd.read_csv(self.hp.dataset.csv_path) if train else pd.read_csv(self.hp.dataset.test_csv_path) 
        self.fs = self.hp.dataset.fs
        self.hrtf_obj = SOFA_HRTF_wrapper(path=hp.dataset.hrtf_path) #,var=hp.dataset.var_hrtf)

    def __len__(self):
        return len(self.df)
    
    def preprocess(self,x,fs):
        #cut length
        if fs != self.fs:
            x = F.resample(x, fs, self.fs)
        if x.shape[0]!=1:
            x = x.squeeze()
        if self.hp.dataset.norm_sample:
            x = x/(x.abs().max(dim=-1, keepdim=True).values)
        if x.shape[-1] <= self.hp.dataset.time_len*self.fs:
             #cat zeros
            remain = self.hp.dataset.time_len*self.fs-x.shape[-1]
            x = torch.cat((x,torch.zeros((x.shape[0],remain))),-1)
        else:
            #cut
            x = x[:,:self.hp.dataset.time_len*self.fs] 
        return x
    
    def mix(self,x1,x2,snr_db):
        #add SNR here
        g = self.calc_SNR_gain(x1,x2,snr_db)
        x = x1+(g*x2)
        x = x/(x.abs().max(dim=-1, keepdim=True).values)
        return self.stft_sample(x)
    
    def stft_sample(self,x):
        X = torch.stft(torch.squeeze(x),n_fft=self.hp.stft.fft_length, hop_length=self.hp.stft.fft_hop, window=torch.hann_window(self.hp.stft.fft_length), return_complex=True).to(torch.complex64)
        X[0:2, :] = X[0:2, :] * 0.001
        mx_x = torch.max(torch.max(torch.abs(torch.real(X)) , torch.max(torch.abs(torch.imag(X)) )))
        X = X/mx_x
        return X
    
    def hrtf_preprocces(self,hrtf):
        HRTF = torch.fft.fft(hrtf,self.hp.stft.fft_length).to(torch.complex64)
        HRTF = HRTF[:,1:self.hp.stft.fft_length//2 +1]
        return HRTF

    def get_hrtf(self,az,elev):
        h=self.hrtf_obj.get_hrtf(az,elev,fs=self.hp.dataset.fs)
        h = self.hrtf_preprocces(torch.tensor(h.T))
        return h

    def __getitem__(self, idx):
        line = self.df.iloc[idx]
        # x1,fs_x1 = torchaudio.load(line['speaker_1'])
        speaker_1 =line['speaker_1']
        az1=line['az_1']
        elev1=line['elev_1']
        speaker_2=line['speaker_2']
        az2=line['az_2']
        elev2=line['elev_2']
        sir = float(line['sir']) 
        return speaker_1,az1,elev1,speaker_2,az2,elev2,sir